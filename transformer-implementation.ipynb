{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport random\nfrom math import inf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T01:26:09.024046Z","iopub.execute_input":"2024-12-06T01:26:09.025076Z","iopub.status.idle":"2024-12-06T01:26:09.029424Z","shell.execute_reply.started":"2024-12-06T01:26:09.025038Z","shell.execute_reply":"2024-12-06T01:26:09.028422Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def getPositionalEncoding(model_dimension, indexes):\n    positional_encodings = np.zeros((indexes.shape[0], indexes.shape[1], model_dimension))\n    for i in range(indexes.shape[0]):\n      for j in range(indexes.shape[-1]):\n          for k in range(model_dimension//2):\n              denominator = 10000**(2*k/model_dimension)\n              positional_encodings[i,j,2*k] = np.sin(indexes[i,j]/denominator)\n              positional_encodings[i,j,2*k+1] = np.cos(indexes[i,j]/denominator)\n\n    return positional_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T01:26:29.604559Z","iopub.execute_input":"2024-12-06T01:26:29.604974Z","iopub.status.idle":"2024-12-06T01:26:29.961101Z","shell.execute_reply.started":"2024-12-06T01:26:29.604941Z","shell.execute_reply":"2024-12-06T01:26:29.960232Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, n_heads, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.depth = d_model // n_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.final_dense = tf.keras.layers.Dense(d_model, activation='relu')\n\n    def split_heads(self, x, batch_size):\n        \"\"\"\n        input shape: batch_size x seq_lenth x d_model\n        intermediate shape: batch_size x seq_length/depth x num_heads x depth\n        output shape: batch_size x num_heads x seq_length/depth x depth\n        \"\"\"\n        x = tf.reshape(x, [batch_size, -1, self.n_heads, self.depth])\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        return x\n\n    def compare_tensors(self, q, k):\n        return tf.reduce_sum(tf.cast(tf.equal(q, k), dtype='int32')).numpy().tolist() == q.shape[0]*q.shape[1]*q.shape[2]\n\n    def create_padding_mask(self, seq):\n        # Identify positions where the token ID is 0 (padding)\n        return tf.cast(tf.math.equal(seq, 0), tf.float32) * 1e-9\n        \n    def create_look_ahead_mask(self, seq):\n        mask = 1 - tf.linalg.band_part(tf.ones(seq.shape), -1, 0)\n        return mask  # (seq_len, seq_len)\n    \n    def combine_masks(self, padding_mask, look_ahead_mask):\n        return tf.maximum(padding_mask, look_ahead_mask)\n\n    def call(self, q, k, v): \n        if self.compare_tensors(q, k):\n            attention_type = 'auto_attention'\n        else:\n            attention_type = 'cross_attention'\n            \n        #linear combination with inputs\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        #splitting heads for multihead attention\n        batch_size = q.shape[0]\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        #calculating attention\n        scale = self.d_model**0.5\n        qk_product = tf.matmul(q, k, transpose_b=True) / scale\n\n        if attention_type == 'auto_attention':\n            mask = create_padding_mask(qk_product)\n        elif attention_type == 'cross_attention':\n            padding_mask = self.create_padding_mask(qk_product)\n            look_ahead_mask = self.create_look_ahead_mask(qk_product)\n            mask = self.combine_masks(padding_mask, look_ahead_mask)\n            \n        qk_product += mask\n\n        qk_product = tf.nn.softmax(qk_product)\n\n        attention_result = tf.matmul(qk_product, v)\n\n        #return the values for the initial shape before splitting\n        pre_output = tf.transpose(attention_result, perm=[0,2,1,3])\n        pre_output = tf.reshape(pre_output, [batch_size, -1, self.d_model])\n\n        #linear combination with non linear activation\n        output = self.final_dense(pre_output)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T02:10:59.371985Z","iopub.execute_input":"2024-12-06T02:10:59.372351Z","iopub.status.idle":"2024-12-06T02:10:59.386596Z","shell.execute_reply.started":"2024-12-06T02:10:59.372320Z","shell.execute_reply":"2024-12-06T02:10:59.385449Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.norm_1 = tf.keras.layers.LayerNormalization()\n        self.norm_2 = tf.keras.layers.LayerNormalization()\n        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25),\n                                        tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25)])\n\n    def call(self, x):\n        auto_attention_result = self.mha(x, x, x)\n        auto_attention_result = self.norm_1(auto_attention_result + x)\n\n        encoder_result = self.ffn(auto_attention_result)\n        encoder_result = self.norm_2(auto_attention_result + encoder_result)\n\n        return encoder_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T02:11:03.612054Z","iopub.execute_input":"2024-12-06T02:11:03.612462Z","iopub.status.idle":"2024-12-06T02:11:03.619952Z","shell.execute_reply.started":"2024-12-06T02:11:03.612426Z","shell.execute_reply":"2024-12-06T02:11:03.618886Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.masked_mha = MultiHeadAttention(d_model, num_heads)\n        self.norm_1 = tf.keras.layers.LayerNormalization()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.norm_2 = tf.keras.layers.LayerNormalization()\n\n        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25),\n                                        tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25)])\n\n        self.norm_3 = tf.keras.layers.LayerNormalization()\n\n    def call(self, x, encoder_result):\n        auto_attention_result = self.masked_mha(x, x, x)\n        auto_attention_result = self.norm_1(auto_attention_result + x)\n\n        cross_attention_result = self.mha(auto_attention_result, encoder_result, encoder_result)\n        cross_attention_result = self.norm_2(cross_attention_result + auto_attention_result)\n\n        ffn_result = self.ffn(cross_attention_result)\n        ffn_result = self.norm_3(cross_attention_result + auto_attention_result)\n\n        return ffn_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T02:11:07.651981Z","iopub.execute_input":"2024-12-06T02:11:07.652784Z","iopub.status.idle":"2024-12-06T02:11:07.661512Z","shell.execute_reply.started":"2024-12-06T02:11:07.652746Z","shell.execute_reply":"2024-12-06T02:11:07.660430Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, d_model, n_heads, vocab_size, **kwargs):\n        super(Transformer, self).__init__(**kwargs)\n        if d_model % n_heads != 0:\n            raise ValueError(\"Number of heads is not a divisor of the dimension model.\")\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.depth = d_model//n_heads\n        self.vocab_size = vocab_size\n\n        self.encoder = Encoder(d_model, n_heads)\n        self.decoder = Decoder(d_model, n_heads)\n\n        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25),\n                                        tf.keras.layers.Dense(d_model, activation='relu'),\n                                        tf.keras.layers.Dropout(0.25)],\n                                        tf.keras.layers.Dense(vocab_size, activation='softmax'))\n\n    def call(self, x):\n        x_enc, x_dec = x\n        \n        encoder_result = self.encoder(x_enc)\n        decoder_result = self.decoder(x_dec, encoder_result)\n\n        probabilities = self.ffn(decoder_result)\n\n        return probabilities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T02:33:46.453785Z","iopub.execute_input":"2024-12-06T02:33:46.454757Z","iopub.status.idle":"2024-12-06T02:33:46.463037Z","shell.execute_reply.started":"2024-12-06T02:33:46.454716Z","shell.execute_reply":"2024-12-06T02:33:46.461889Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"D_MODEL = 512\nN_HEADS = 16\nVOCAB_SIZE = 100\nTEXT_SIZE = 20\nBATCH_SIZE = 4\nx_enc = np.array([[random.randint(0, VOCAB_SIZE) for _ in range(TEXT_SIZE)] for _ in range(BATCH_SIZE)])\nx_dec = np.array([[random.randint(0, VOCAB_SIZE) for _ in range(TEXT_SIZE)] for _ in range(BATCH_SIZE)])\nx_enc = getPositionalEncoding(D_MODEL, x_enc)\nx_dec = getPositionalEncoding(D_MODEL, x_dec)\npadding_mask = create_padding_mask(x_enc)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}